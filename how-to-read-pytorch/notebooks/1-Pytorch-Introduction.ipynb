{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Read Pytorch\n",
    "===================\n",
    "\n",
    "These five python notebooks are an introduction to core pytorch idioms.\n",
    "\n",
    "Pytorch is a numerical library that makes it very convenient to train deep networks on GPU hardware. It introduces a new programming vocabulary that takes a few steps beyond regular numerical python code. Although pytorch code can look simple and concrete, much of of the subtlety of what happens is invisible, so when working with pytorch code it helps to thoroughly understand the runtime model.\n",
    "\n",
    "For example, consider this code:\n",
    "\n",
    "```\n",
    "torch.nn.cross_entropy(model(images.cuda()), labels.cuda()).backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "It looks like it computes some function of `images` and `labels` without storing the answer.  But actually the purpose of this code is to update some hidden parameters that are not explicit in this formula.  This line of code moves batches of image and label data from CPU to the GPU; runs a neural network to make a prediction; constructs a computation graph describing how the loss depends on the network parameters; annotates every network parameter with a gradient; then finally it runs one step of optimization to adjust every parameter of the model.  During all this, the CPU does not see any of the actual answers.  That is intentional for speed reasons.  All the numerical computation is done on the GPU asynchronously and kept there.\n",
    "\n",
    "The berevity of the code is what makes pytorch code fun to write.  But it also reflects why pytorch can be so fast even though the python interpreter is so slow. Although the main python logic slogs along sequentially in a single very slow CPU thread, just a few python instructions can load a huge amount of work into the GPU.  That means the program can keep the GPU busy churning through massive numerical computations, for most part, without waiting for the python interpreter.\n",
    "\n",
    "Is is worth understanding five core idioms that work together to make this possible.  This tutorial has five Colab notebooks, one for each topic:\n",
    "\n",
    " 1. GPU Tensor arithmetic ([this notebook on colab](https://colab.research.google.com/github/davidbau/how-to-read-pytorch/blob/master/notebooks/1-Pytorch-Introduction.ipynb)): the notation for manipulating n-dimensional arrays of numbers on CPU or GPU.\n",
    " 2. [Autograd](./2-Pytorch-Autograd.ipynb): how to build a tensor computation graph and use it to get derivatives of any scalar with respect to any input.\n",
    " 3. [Optimization](./3-Pytorch-Optimizers.ipynb): ways to update tensor parameters to reduce any computed objective, using autograd gradients.\n",
    " 4. [Network modules](./4-Pytorch-Modules.ipynb): how pytorch represents neural networks for convenient composition, training, and saving.\n",
    " 5. [Datasets and Dataloaders](./5-Pytorch-Dataloader.ipynb): for efficient multithreaded prefetching of large streams of data.\n",
    "\n",
    "The key ideas are illustrated with small, illustrated, hackable examples, and there are links to other reference material and resources.\n",
    "\n",
    "All the notebooks can be run on Google Colab where some GPU compuation can be used for free, or they can be run on your own local Jupyter notebook server.\n",
    "\n",
    "The examples should all work with python 3.5 or newer and pytorch 1.0 or newer.\n",
    "\n",
    "The original [code on github can be found here](https://github.com/davidbau/how-to-read-pytorch).\n",
    "\n",
    "--- [*David Bau, July 2020*](http://davidbau.com/archives/2020/07/05/davids_tips_on_how_to_read_pytorch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 1: pytorch Tensors\n",
    "===============\n",
    "\n",
    "The first big trick for doing math fast on a modern computer is to do giant array operations all at once.  To faciliate this, pytorch provides a [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) class that is a lookalike to the older python numerical library [`numpy.ndarray`](https://numpy.org/doc/1.18/reference/arrays.ndarray.html).  Just like a numpy `ndarray`, the pytorch `Tensor` stores a d-dimensional array of numbers, where d can be zero or more, and where the contained numbers can be any of the usual selection of float or integer types.  Pytorch is designed to feel just like numpy: almost all the numpy operations are also available on torch tensors. But if something is missing, torch tensors can be directly converted to and from numpy using `x.numpy()` and `torch.from_numpy(a)`. So what is different and why did the pytorch authors bother to reimplement this whole library?\n",
    "\n",
    "**There are two things that pytorch Tensors have that numpy arrays lack:**\n",
    "\n",
    " 1. pytorch Tensors can live on either **GPU or CPU** (numpy is cpu-only).\n",
    " 2. pytorch can automatically track tensor computations to enable **automatic differentiation**.\n",
    "\n",
    "In the following sections on this page we talk about the basics of the Tensor API as well as point (1) - how to work with GPU and CPU tensors.  A discussion of (2) can be found in the next notebook, [2. Autograd](https://colab.research.google.com/github/davidbau/pytorch-tutorial/blob/master/notebooks/2-Pytorch-Autograd.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic operations in the Tensor API\n",
    "----------------------------------\n",
    "\n",
    "Pytorch is not very different from numpy, although the pytorch API has more convenience methods such as `x.clamp(0).pow(2)` (supporting a chained method style, as is popular in Javascript libraries). So code is often shorter in pytorch.  A brief overview:\n",
    "\n",
    "**Elementwise operations.** Most tensor operations are simple (embarassingly parallelizable) elementwise operations, where the same math is done on every element of the array.  `x+y`, `x*y`, `x.abs()`, `x.pow(3)`, etc.  Unlike Matlab, `*` is for element-wise multiplication, not matrix-multiplication.\n",
    "\n",
    "**Copy semantics by default.** Almost all operations, including things like `x.sort()`, return a new copy of the tensor without overwriting the input tensors.  The exceptions are functions that end in an underscore such as `x.mul_(2)` which doubles the contents of x in-place.\n",
    "\n",
    "**Common reduction operations.**  There are some common operations such as `max`, `min`, `mean`, `sum` that reduce the array by one or more dimension. In pytorch, you can specify which dimension you want to reduce by passing the argument `dim=n`.\n",
    "\n",
    "**Why does min return two things?** Note that `[data, indexes] = x.sort(dim=0)` and `[vals, indexes] = x.min(dim=0)` return the pair of both the answer and the index values, so you do not need to separately recompute `argsort` or `argmin` when you need to know where the min came from.\n",
    "\n",
    "**What about linear algebra?** It's there.  `torch.mm(a,b)` is matrix multiplication, `torch.inv(a)` inverts, `torch.eig(a)` gets eigenvalues, etc.\n",
    "\n",
    "The other thing to know is that pytorch tends to be very fast, often much faster than numpy even on CPU, because its implementation is aggressively parallelized behind-the-scenes.  Pytorch is willing to use multiple threads in situations where numpy just uses one.\n",
    "\n",
    "See the [reference for Tensor methods](https://pytorch.org/docs/stable/tensors.html#torch.Tensor) for what comes built-in.  A simple demo of some vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, numpy, torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make a vector of 101 equally spaced numbers from 0 to 5.\n",
    "x = torch.linspace(0, 5, 101)\n",
    "\n",
    "# Print the first five things in x.\n",
    "print(x[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Print the last five things in x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some vector computations.\n",
    "y1, y2 = x.sin(), x ** x.cos()\n",
    "y3 = y2 - y1\n",
    "y4 = y3.min()\n",
    "\n",
    "# Print and plot some answers.\n",
    "print(f'The shape of x is {x.shape}')\n",
    "print(f'The shape of y1=x.sin() is {y1.shape}')\n",
    "print(f'The shape of y2=x ** x.cos() is {y2.shape}')\n",
    "print(f'The shape of y3=y2 - y1 is {y3.shape}')\n",
    "print(f'The shape of y4=y3.min() is {y4.shape}, a zero-d scalar')\n",
    "\n",
    "plt.plot(x, y1, 'red', x, y2, 'blue', x, y3, 'green')\n",
    "plt.axhline(y4, color='green', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot y3 clamped between 0.0 and 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subscripts and multiple dimensions\n",
    "----------------------------------\n",
    "\n",
    "Pytorch code is full of multidimensional arrays.  The key to reading this kind of code is stopping to think about the careful, sometimes tangled, use of multiple array subscripts.\n",
    "\n",
    "**Slicing.** As normal in python, you can use `[min:max:stride]` to slice ranges, and multidimensional subscripts like `x[2,0,1,9]` work as you would expect (selecting the 9th entry of the of the 1st of the 0th of the 2nd entry of `x`; and can be used with slices like `x[0:3,2,:,:]`.  The special slice `:` selects the whole range in that dimension.\n",
    "\n",
    "**Unsqueezing to add a dimension, and broadcasting.** While a single integer subscript like `x[0]` eliminates a dimension, the special subscript `x[None]` does the reverse and adds an extra dimension of size one.\n",
    "\n",
    "An extra dimension of size one is more useful than you might imagine, because pytorch (similar to numpy) can combine different-shaped arrays as long as the shape differences appear only on dimensions of size one by **broadcasting** the singleton dimensions.  An example that uses broadcasting to calculate an outer product is illustrated below.\n",
    "\n",
    "**Fancy indexing.** Lots more can be done by passing numerical arrays or boolean array masks as subscripts.  The reshuffling possibilities can get quite intricate; the rules are modeled on the capabilties in numpy.  For details see [Numpy fancy indexing](https://numpy.org/doc/stable/user/basics.indexing.html).\n",
    "\n",
    "Here is a demonstration of simple tensor reshaping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make an array of normally distributed randoms.\n",
    "m = torch.randn(2, 5).abs()\n",
    "print(f'm is {m}, and m[1,2] is {m[1,2]}\\n')\n",
    "print(f'column zero, m[:,0] is {m[:,0]}')\n",
    "print(f'row zero m[0,:] is {m[0,:]}\\n')\n",
    "dot_product = (m[0,:] * m[1,:]).sum()\n",
    "print(f'The dot product of rows (m[0,:] * m[1,:]).sum() is {dot_product}\\n')\n",
    "outer_product = m[0,:][None,:] * m[1,:][:,None]\n",
    "print(f'The outer product of rows m[0,:][None,:] * m[1,:][:,None] is:\\n{outer_product}')\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(5, 5), dpi=100)\n",
    "def color_mat(ax, m, title):\n",
    "    ax.set_title(title)\n",
    "    ax.imshow(m, cmap='hot', vmax=1.5, interpolation='nearest')\n",
    "    ax.get_xaxis().set_ticks(range(m.shape[1]))\n",
    "    ax.get_yaxis().set_ticks(range(m.shape[0]))\n",
    "color_mat(ax1, m, 'm[:,:]')\n",
    "color_mat(ax2, m[0,:][None,:], 'm[0,:][None,:]')\n",
    "color_mat(ax3, m[1,:][:,None], 'm[1,:][:,None]')\n",
    "color_mat(ax4, outer_product, 'm[0,:][None,:] * m[1,:][:,None]')\n",
    "fig.tight_layout()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use `torch.mm` to compute `outer_product` and `dot_product`.\n",
    "\n",
    "Explain to yourself why order matters when using torch.mm but not when using `*`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devices and types\n",
    "-----------------\n",
    "\n",
    "One of the big reasons to use pytorch instead of numpy is that pytorch can do computations on the GPU.  But because moving data on and off of a GPU device is more expensive than keeping it within the device, pytorch treats a Tensor's **computing device** as pseudo-type that requires explicit declaration and explicit conversion.  Here are some things to know about pytorch devices and types:\n",
    "\n",
    "**Single precision CPU default.** By default a torch tensor will be stored on the CPU and will store single-precision 32-bit `torch.float` values.\n",
    "\n",
    "**Specifying data type.** To store a different data type such as integers, use the argument `dtype=torch.long` when you create the Tensor.  For example, `z = torch.zeros(10, dtype=torch.long)`.  This is similar to numpy with minor differences.  See the [Tensor reference](https://pytorch.org/docs/stable/tensors.html) for all the types.\n",
    "\n",
    "**Specifying GPU.** To store the tensor on the GPU, specify `device='cuda'` when you make it, for example `identity_matrix = torch.eye(5, device='cuda')`.  (Instead `device='cpu'` indicates the default CPU storage).\n",
    "\n",
    "Even on a multi-GPU machine it is fine to pretend there is only one GPU.  Setting the environment variable `CUDA_VISIBLE_DEVICES=3` before you start the program will set up the process to see GPU\\#3 as the only visible GPU when it runs.\n",
    "\n",
    "As an aside, in principle you could instead target one of many GPUs with `device='cuda:3'`, but if you want to use multiple GPUs for the same computation your best bet is to a use a multiprocess utility class that manages data distribution between forked processes automatically, while each python process touches only one GPU.  When this becomes an issue, read the [DistributedDataParalllel docs](https://pytorch.org/docs/stable/distributed.html).\n",
    "\n",
    "**Copying a tensor to a different device or type.** You cannot directly combine tensors that are on different devices (e.g., GPU vs CPU or different GPUs); this is similar to how most different-data-type combinations are also prohibited. In both cases you will need to convert types and move devices explicitly to make tensors compatible before combining them.  The `x.to(y.device)` or `x.to(y.dtype)` function can be used to do the conversion.\n",
    "\n",
    "There are also commonly-used convenience synonyms `x.cpu()`, `x.cuda()`, `x.float()`, `x.long()`, etc. for making a copy of `x` with the specified device or type.  There is a bit of cost, so move data only when needed.\n",
    "\n",
    "**GPU rounding is nondeterministic.** Computationally the GPU is **not** perfectly equivalent to the CPU.  To speed parallelization, the GPU does not do associative operations such as summations in a deterministic sequential order.  Since changing the order of summations can alter rounding behavior in fixed-precision arithmetic, GPU rounding can be  different from CPU results an even nondeterministic.  When the numerical algorithm is well-behaved, the difference should be small enough that you do not care, but you should know it is different. You can see this gap in the code example below.\n",
    "\n",
    "**float is fastest.** All commodity GPU hardware is fast at single-precision 32-bit floating-point math, about 20x CPU speed.  Be aware that only expensive cards are fast at 64-bit double-precision math. If you change `torch.float` in the below example to `torch.double` on an Nvidia Titan or consumer card without hardware double-precision support, you will slow down to just-slightly-faster-than-CPU speeds.  Similarly 16-bit `torch.half` or `torch.bfloat16` or other cool options will only be faster on newer hardware, and with these data types you need to take care that the reduced precision is not damaging your results.\n",
    "\n",
    "So `float` is the default and usually the best.\n",
    "\n",
    "Also note that some operations (like linear algebra) are floating-point only and cannot be done on integers.\n",
    "\n",
    "An example of some CPU versus GPU speed comparisons is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, time\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Here is a demonstration of moving data between GPU and CPU.\n",
    "# We multiply a batch of vectors through a big linear opeation 10 times\n",
    "r = torch.randn(1024, 1024, dtype=torch.float)\n",
    "x = torch.randn(32768, 1024, dtype=r.dtype)\n",
    "iterations = 10\n",
    "\n",
    "def time_iterated_mm(x, matrix):\n",
    "    start = time.time()\n",
    "    result = 0\n",
    "    for i in range(iterations):\n",
    "        result += torch.mm(matrix, x.to(matrix.device).t())\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start\n",
    "    return elapsed, result.cpu()\n",
    "\n",
    "cpu_time, cpu_result = time_iterated_mm(x.cpu(), r.cpu())\n",
    "print(f'time using the CPU alone: {cpu_time:.3g} seconds')\n",
    "\n",
    "mixed_time, mixed_result = time_iterated_mm(x.cpu(), r.cuda())\n",
    "print(f'time using GPU, moving data from CPU: {mixed_time:.3g} seconds')\n",
    "\n",
    "pinned_time, pinned_result = time_iterated_mm(x.cpu().pin_memory(), r.cuda())\n",
    "print(f'time using GPU on pinned CPU memory: {pinned_time:.3g} seconds')\n",
    "\n",
    "gpu_time, gpu_result = time_iterated_mm(x.cuda(), r.cuda())\n",
    "print(f'time using the GPU alone: {gpu_time:.3g} seconds')\n",
    "\n",
    "plt.figure(figsize=(4,2), dpi=150)\n",
    "plt.ylabel('iterations per sec')\n",
    "plt.bar(['cpu', 'mixed', 'pinned', 'gpu'],\n",
    "        [iterations/cpu_time,\n",
    "         iterations/mixed_time,\n",
    "         iterations/pinned_time,\n",
    "         iterations/gpu_time])\n",
    "plt.show()\n",
    "\n",
    "print(f'Your GPU is {cpu_time / gpu_time:.3g}x faster than CPU'\n",
    "      f' but only {cpu_time / mixed_time:.3g}x if data is repeatedly copied from the CPU')\n",
    "print(f'When copying from pinned memory, speedup is {cpu_time / pinned_time:.3g}x')\n",
    "print(f'Numerical differences between GPU and CPU: {(cpu_result - gpu_result).norm() / cpu_result.norm()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the benchmark using type `torch.double`.   What does that tell you about your GPU hardware?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance tips\n",
    "----------------\n",
    "\n",
    "**GPU operations are async.** When pytorch operates on GPU tensors, the python code does not wait for computations to complete. Sp GPU calculations get queued up, and they will be done as quickly as possible in the background while your python is free to work on other things like loading the next batch of training data.\n",
    "\n",
    "**Moving data to cpu waits for computations.** You do not need to worry about the GPU asynchrony, because as soon as you actually ask to look at the data, e.g., when you move GPU data to CPU (or print it or save it), pytorch will block and wait for the GPU operations to finish computing what you need before proceeding. The call seen above to `torch.cuda.synchronize()` flushes the GPU queue without requesting the data, but you will not need to do this unless you are doing performance timing.\n",
    "\n",
    "**Pinned memory transfers are async and faster.** Copying data from CPU to GPU can be sped up if the CPU data is put in pinned memory (i.e., at a fixed non-swappable block of RAM).  Therefore when data loaders gather together lots of CPU data that is destined for the GPU, they should be configured to stream their results into pinned memory. See the performance comparison above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch Tensor dimension-ordering conventions\n",
    "---------------------------------------------\n",
    "\n",
    "**Multidimensional data convention.** As soon as you have more than one dimension, you need to decide how to order the axes.  To reduce confusion, most data processing follows the same global convention. In particular, much image-related data in pytorch is four dimensional, and the dimensions are ordered like this: `data[batch_index, channel_index, y_position, x_position]`, that is:\n",
    "\n",
    "* Dimension 0 is used to index separate images within a batch.\n",
    "* Dimension 1 indexes channels within an image representation (e.g., 0,1,2 = R,G,B, or more dims for more channels).\n",
    "* Dimension 2 (if present) indexes the row position (y-value, starting from the top)\n",
    "* Dimension 3 (if present) indexes the column position (x-value, starting from the left)\n",
    "\n",
    "There a way to remember this ordering: adjacent entries that vary only in the last dimensions are stored physically closer in RAM; since they are often combined with each other, this could help with locality, whereas the first (batch) dimension usually just groups separate independent data points which are not combined much, so they do not need to be physically close.\n",
    "\n",
    "Stream-oriented data without grid geometry will drop the last dimensions, and 3d grid data will be 5-dimensional, adding a depth z before y.  This same 4d-axis ordering convention is also seen in caffe and tensorflow.\n",
    "\n",
    "Separate tensors can be put together into a single batch tensor using `torch.cat([a, b, c])` or `torch.stack([a, b, c])`.  (The difference: `cat` doesn't add any new dimensions but just concatenates along the existing 0th dimension.  `stack` adds a new 0th dimension for the batch.)\n",
    "\n",
    "**Multidimensional linear operation convention.** When storing matrix weights or convolution weights, linear algebra conventions are followed\n",
    "* Dimension 0 (number of rows) matches the output channel dimension\n",
    "* Dimension 1 (number of columns) matches the input channel dimension\n",
    "* Dimension 2 (if present) is the convolutional kernel y-dimension\n",
    "* Dimension 3 (if present) is the convolutional kernel x-dimension\n",
    "\n",
    "Since this convention assumes channels are arranged in different rows whereas the data convention puts different batch items in different rows, some axis transposition is often needed before applying linear algebra to the data.\n",
    "\n",
    "**Permute and view reshape an array without moving memory.** The `permute` and `view` methods are useful for rearranging, flattening, and unflatteneing axes. `x.permute(1,0,2,3).view(x.shape[1], -1)`.  They just alter the view of the block of numbers in memory without moving any of the numbers around, so they are fast.\n",
    "\n",
    "**Reshaping sometimes needs copying.** Some sequences of axis permutations and flattenings cannot be done without copying the data into the new order in memory; the `x.contiguous()` method copies the data iinto the natural order given by the current view; also `x.reshape()` is similar to `view` but will makea copy if necessary so you do not need to think about it.  See [the Tensor.view method documentation](https://pytorch.org/docs/master/tensors.html#torch.Tensor.view).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Use `torch.randn` to create a four-dimensional tensor `x` of size (2,3,4,5), which could store two 5x4 RGB images.\n",
    "\n",
    "Then print three things:\n",
    " * print `x`.\n",
    " * Use `x.permute` to switch the horizontal and vertical (last two) dimensions.\n",
    " * Use `x.view` to see each image as a flat vector of 60 numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [On to topic 2: Autograd &rightarrow;](2-Pytorch-Autograd.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}